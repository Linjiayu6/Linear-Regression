>[Logistic Regression 随机梯度下降算法 - 优化](https://cuijiahua.com/blog/2017/11/ml_7_logistic_2.html)

- 进阶用法: practice_2.py

# 1. Recap
- 详见 README_1.md

## 1.1 提出问题? 
practice_1.py中, 有100个数据样本, 对于每一次的计算(h(x) = w0x0 + w1x1 + w2w2)
也就是说, 每次计算, 都需要100 * 3次的乘法 和 100 * 2 次的加法 = 500次的运算
如果我们有一千万个数据样本呢? 岂不是爆炸了.....

- 如何优化计算复杂度?
- 如何优化处理时间?
- ...

**总之, 如何对算法进行改进?**


# 2. 优化算法
随机梯度下降为什么比梯度下降快？

**随机梯度下降每次只采用部分样本计算梯度，梯度下降每次使用所有样本计算梯度。**


"""
不能笼统的说随机梯度下降比梯度下降速度快，**多数情况下**，**mini-batch会比sgd收敛要速度快**。在训练样本多的情况下，sgd比gd的收敛速度快.
- 一个原因是gd要通过所有样本计算梯度，而**sgd通过一个样本计算一个近似的梯度**，本身计算量就会小非常多，如果考虑到gd计算时有可能不是所有的样本都能到内存中，速度就会减少得更慢。

至于每次计算近似梯度，那么迭代次数会不会大量增加了？ 首先sgd会比较快的收敛到一个还不错的解，如果要收敛到精确解就会比较困难，幸运的是在机器学习中，**我们不需要在训练集上一个最优解**，很多时候甚至需要early stop。至于为什么迭代次数不需要增加非常多，一个解释是样本中很多样本是重复的，举个极端的例子，所有训练样本都是一样，那么sgd和gd算出来的梯度是一样的，这样迭代次数也和gd的迭代次数是一样的

作者：童明
链接：[链接](https://www.zhihu.com/question/40892922/answer/93950339)
"""

# 3. 问题
- alpha = 0.001 + 4 / (1 + j + i)? 为什么如此, 开始步子迈的大4.01, 后面收敛到0.02?
- 每一次迭代, 都对每个样本随机的选取, 训练权重, 并且不重复, 目的是为什么? 迭代次数不是增加了吗?
- 